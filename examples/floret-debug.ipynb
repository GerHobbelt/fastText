{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59c3ef8-8ed6-429e-a68b-9dd5141b13f8",
   "metadata": {},
   "source": [
    "This notebook contains code that can generate charts that might help debug and understand floret embeddings. This notebook is meant as an internal tool.\n",
    "\n",
    "## Import \n",
    "\n",
    "Let's start by importing the binary files with floret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa1dbe-6e8a-43f2-b7f4-88aa0a020bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'spacy~=3.4.0' floret pandas altair sklearn tabulate https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.0/en_core_web_lg-3.4.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c41502-6c5c-49b9-9f6e-9856d124d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import floret \n",
    "\n",
    "model_fl = floret.load_model(\"en_vectors_floret_md.bin\")\n",
    "model_ft = floret.load_model(\"en_vectors_fasttext.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6ce90-f60a-4d34-9399-932b1eb8e366",
   "metadata": {},
   "source": [
    "The code that follows can generate the subtoken charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71c002-1302-4638-802f-0bc97e9e558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clean_subtokens(subtokens):\n",
    "    \"\"\"This ensures consistency between fasttext/floret.\"\"\"\n",
    "    first_tok = subtokens[0]\n",
    "    if first_tok[0] == \"<\":\n",
    "        if first_tok[-1] == \">\":\n",
    "            return subtokens[1:]\n",
    "    if first_tok[0] != \"<\":\n",
    "        if first_tok[-1] != \">\":\n",
    "            return subtokens[1:]\n",
    "\n",
    "    return subtokens\n",
    "\n",
    "\n",
    "def plot_similarity_altair(w1, w2, model, title, return_df=False):\n",
    "    \"\"\"Handles plotting and cosine similarity calculation.\"\"\"\n",
    "    w1_subtokens, _ = model.get_subwords(w1)\n",
    "    w2_subtokens, _ = model.get_subwords(w2)\n",
    "    w1_subtokens, w2_subtokens = clean_subtokens(w1_subtokens), clean_subtokens(w2_subtokens)\n",
    "    w1_x = [model.get_word_vector(s) for s in w1_subtokens]\n",
    "    w2_x = [model.get_word_vector(s) for s in w2_subtokens]\n",
    "    similarities = cosine_similarity(w1_x, w2_x)\n",
    "    data = [] \n",
    "    for i, wi in enumerate(w1_subtokens):\n",
    "        for j, wj in enumerate(w2_subtokens):\n",
    "            data.append({\n",
    "                'x': wi,\n",
    "                'y': wj,\n",
    "                'sim': similarities[i, j]\n",
    "            })\n",
    "    pltr = pd.DataFrame(data)\n",
    "    if return_df:\n",
    "        return pltr\n",
    "    return (alt.Chart(\n",
    "                pltr,\n",
    "                title=title,\n",
    "            ).mark_rect().encode(\n",
    "                x=alt.X('x', sort=w1_subtokens, title=w1),\n",
    "                y=alt.Y('y', sort=w2_subtokens, title=w2),\n",
    "                color=alt.Color('sim', scale=alt.Scale(scheme=\"redblue\", domain=(.7, -.4))),\n",
    "                tooltip=[\n",
    "                    alt.Tooltip('y', title='Word X'), \n",
    "                    alt.Tooltip('x', title='Word Y'),\n",
    "                    alt.Tooltip('sim', title='Cosine Similarity')\n",
    "                ]\n",
    "            ).properties(width=200, height=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a12f3-accc-43a4-942d-6a85772ebabe",
   "metadata": {},
   "source": [
    "## Plot all the charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b625be5-2e87-4888-b1e8-4a3ca009bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot_similarity_altair(\"preadolescent\", \"youthful\", model=model_fl,title=\"\")\n",
    "p2 = plot_similarity_altair(\"circuitry\", \"dinosaur\", model=model_fl, title=\"\")\n",
    "p3 = plot_similarity_altair(\"plasmagraphy\", \"radiology\", model=model_fl, title=\"\")\n",
    "p4 = plot_similarity_altair(\"machinery\", \"mechanism\", model=model_fl, title=\"\")\n",
    "\n",
    "p5 = plot_similarity_altair(\"preadolescent\", \"youthful\", model=model_ft,title=\"\")\n",
    "p6 = plot_similarity_altair(\"circuitry\", \"dinosaur\", model=model_ft, title=\"\")\n",
    "p7 = plot_similarity_altair(\"plasmagraphy\", \"radiology\", model=model_ft, title=\"\")\n",
    "p8 = plot_similarity_altair(\"machinery\", \"mechanism\", model=model_ft, title=\"\")\n",
    "\n",
    "(p5 | p6 | p7 | p8) & (p1 | p2 | p3 | p4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ea6fe-8708-4e48-a8c2-69b895404571",
   "metadata": {},
   "source": [
    "The top row represents fasttext, the bottom one represents floret. We can see that we still manage to keep the correlation between subtokens intact, despite using a hashing table. \n",
    "\n",
    "## Query\n",
    "\n",
    "You can also learn a lot from looking at the nearest neigbhors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37f1b6-e4e9-4499-9cdc-052ba9945581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c137e171-f388-4770-91ed-e32bafe33fff",
   "metadata": {},
   "source": [
    "The code below allows you to make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625c282-901a-4ace-a85b-21897ecbc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"doomscrolling\"\n",
    "\n",
    "vec = nlp.vocab[query].vector\n",
    "hashes, _, scores = nlp.vocab.vectors.most_similar(np.array([vec]), n=11)\n",
    "if nlp.vocab[query].has_vector:\n",
    "    items = [(nlp.vocab[h].text, s) for h, s in zip(hashes[0], scores[0])][1:]\n",
    "else:\n",
    "    items = [(nlp.vocab[h].text, s) for h, s in zip(hashes[0], scores[0])][:10]\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.DataFrame([{\"floret_word\": t[1], \"floret_score\": t[0]} for t in model_fl.get_nearest_neighbors(query)]),\n",
    "    #pd.DataFrame([{\"fasttext_word\": t[1], \"fasttext_score\": t[0]} for t in model_ft.get_nearest_neighbors(query)]),\n",
    "    pd.DataFrame([{\"spaCy_word\": t[0], \"spaCy_score\": t[1]} for t in items])\n",
    "], axis=1)\n",
    "\n",
    "print(df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87828daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
